# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data about bank marketing phone calls placed to various customers. The objective is to predict whether a user will agree to a term deposit i.e. the value of y column.
The best performing model was a VotingEnsemble model with an accuracy of 0.9173

## Scikit-learn Pipeline
In this pipeline, we start by creating a workspace, experiment and compute cluster to run the experiment.
The next step is to use an SKLearn estimator from Azure ML's Python SDK. The estimator takes as input a training script (train.py) and a compute cluster (created in the first setp).
The training script does the following-
1. Defines a main method which can be used to run the Logistic Regression model from sklearn. This method also accepts two parameters to train the model- the regularization strength and number of iterations.
2. Defines a clean_data method which takes a TabularDataset, creates a pandas dataframe to apply the following clean operations-
	- replace values of columns like jobs, contact, education with categorical values
	- replaces string values like yes, no with boolean values (1,0)
	- replaces months and day_of_week with numerical values
	It then return two data frames- x (input features) and y (the value to be predicted)
3. Fetches the data and creates a TabularDataset, cleans it using clean_data, splits it into a training set and test set.

We then define the parameters to be tuned using RandomParameterSampling and early termination policy (BanditPolicy)
What are the benefits of the parameter sampler you chose?
RandomParameterSampling can be used with both discrete and continuous parameters and supports early termination policy. It can be used to do an initial pass of the parameter space and then a more a refined search.
BanditPolicy has the benefit that it allows termination if the primary metric is less than the best performing model. This is governed by the slack faction factor that the user provides.

## AutoML
The model generated by AutoML is VotingEnsemble with the following parameters- 0.07142857142857142, 0.35714285714285715, 0.14285714285714285, 0.07142857142857142, 0.14285714285714285, 0.14285714285714285, 0.07142857142857142. This top features influencing this model were duration of the call and employment status of the user.

## Pipeline comparison
The HyperDrive experiment which used sklearn's LogisticRegression had an accuracy of 0.9107 while the AutoML model had an accuracy of 0.9173. The model selected by AutoML was VotingEnsemble. The difference in performance is due to the fact that in AutoML, multiple algorithms were used while in the HyperDrive experiment, a single classification algorithm was used with multiple parameters.

As part of the HyperDrive experiment, I tried multiple combinations of parameter sampling and stopping policies to find a better model but could not get a better accuracy. results from multiple runs are summarized in the table below-

Sampling Type| Early Termination Policy | C | Max Iterations | Accuracy
------------ | ------------- | ------------ | ------------- | -------------
RandomParameterSampling | BanditPolicy | 0.173 | 100 | 0.9107
RandomParameterSampling | MediaStoppingPolicy | 0.66 | 100 | 0.9107
BayesianParameterSampling | NA | 0.912 | 250 | 0.9107

## Future work
For future experiments, we could try to increase the timeout of AutoML runs.

